{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770d5c2a",
   "metadata": {},
   "source": [
    "# Pipeline con LangChain: PDF → Teacher (ChatGPT) → JSONL → Student (Ollama/Qwen) → Versionado → Knowledge Base\n",
    "\n",
    "Este notebook implementa el flujo **mínimo** usando **LangChain**:\n",
    "\n",
    "1. Extraer texto plano desde un PDF (factura)\n",
    "2. Llamar a **ChatGPT como teacher** con **salida estructurada** (Pydantic)\n",
    "3. Guardar artifacts + generar dataset **JSONL** para fine-tuning local (LoRA)\n",
    "4. Llamar a **Ollama/Qwen como student** (también con salida JSON) y validar con el mismo schema\n",
    "5. Registrar un **knowledge base log** (JSONL + SQLite opcional)\n",
    "\n",
    "> Nota: **Ollama no entrena por API**. Aquí generamos el JSONL para entrenar con una tool local (LoRA) y luego cargar el modelo resultante en Ollama.\n",
    "\n",
    "Generado: 2026-01-05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e473f",
   "metadata": {},
   "source": [
    "## 0) Instalación\n",
    "Instala dependencias principales:\n",
    "- `langchain`, `langchain-core`\n",
    "- `langchain-openai` (ChatGPT teacher)\n",
    "- `langchain-ollama` (Ollama/Qwen student)\n",
    "- `pdfplumber` para extraer texto\n",
    "- `pydantic` para schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install langchain langchain-core langchain-openai langchain-ollama pdfplumber pydantic requests python-dotenv\n",
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a990f",
   "metadata": {},
   "source": [
    "## 1) Configuración\n",
    "- `OPENAI_API_KEY` debe estar en variables de entorno\n",
    "- Ollama debe estar corriendo en `http://localhost:11434`\n",
    "- Cambia `PDF_PATH` si lo necesitas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b375aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, hashlib, datetime\n",
    "import pdfplumber\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "PDF_PATH = Path(\"facturas_compras_sample01.pdf\")\n",
    "\n",
    "# Teacher (OpenAI via LangChain)\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Student (Ollama via LangChain)\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "STUDENT_MODEL = os.getenv(\"OLLAMA_MODEL\", \"qwen2.5:7b-instruct\")\n",
    "\n",
    "RUN_ID = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ARTIFACTS_DIR = Path(\"artifacts\") / RUN_ID\n",
    "KB_DIR = Path(\"knowledge_base\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "KB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PDF exists?:\", PDF_PATH.exists())\n",
    "print(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR.resolve())\n",
    "print(\"KB_DIR:\", KB_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe59f4",
   "metadata": {},
   "source": [
    "## 2) Extraer texto del PDF\n",
    "Si el PDF es escaneado (imagen), `pdfplumber` puede devolver poco texto. Para escaneados, usa OCR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e81d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_pdfplumber(pdf_path: Path) -> str:\n",
    "    parts = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            parts.append(page.extract_text() or \"\")\n",
    "    return \"\\n\\n\".join(parts).strip()\n",
    "\n",
    "raw_text = extract_text_pdfplumber(PDF_PATH)\n",
    "print(\"chars:\", len(raw_text))\n",
    "print(raw_text[:900])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f7093",
   "metadata": {},
   "source": [
    "## 3) Schema Pydantic (teacher/student)\n",
    "Usaremos el mismo schema para:\n",
    "- Validar lo que responde ChatGPT (teacher)\n",
    "- Validar lo que responde Qwen/Ollama (student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ccd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensitivityLabel(BaseModel):\n",
    "    label: int = Field(..., description=\"0=no sensible, 1=sensible\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    signals: List[str] = Field(default_factory=list)\n",
    "    rationale_short: str = Field(..., description=\"Una frase breve, sin pasos intermedios.\")\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "DOC_HASH = sha256_text(raw_text)\n",
    "DOC_HASH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9eaa42",
   "metadata": {},
   "source": [
    "## 4) Prompt base (teacher)\n",
    "Forzamos salida estructurada. Con LangChain lo más limpio es usar `with_structured_output(SensitivityLabel)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_SYSTEM = (\n",
    "    \"Eres un analista experto en protección de datos. \"\n",
    "    \"Clasifica documentos administrativos/financieros por sensibilidad.\"\n",
    ")\n",
    "\n",
    "TEACHER_USER_TEMPLATE = \"\"\"Clasifica el siguiente documento como sensible o no sensible.\n",
    "\n",
    "Reglas:\n",
    "- label=1 si aparece IBAN, NIF/CIF, datos bancarios, datos de pago, identificadores fiscales o información financiera identificable.\n",
    "- label=0 si no aparecen.\n",
    "\n",
    "Devuelve SOLO la estructura (no texto extra).\n",
    "\n",
    "TEXTO:\n",
    "<<<\n",
    "{doc_text}\n",
    ">>>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938daf24",
   "metadata": {},
   "source": [
    "## 5) Teacher con LangChain (ChatOpenAI) + salida estructurada\n",
    "Requiere:\n",
    "- `OPENAI_API_KEY` en env\n",
    "- `langchain-openai`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher (OpenAI) con LangChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def teacher_label_with_langchain(doc_text: str) -> SensitivityLabel:\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"Falta OPENAI_API_KEY en el entorno.\")\n",
    "\n",
    "    llm = ChatOpenAI(model=TEACHER_MODEL, temperature=0)\n",
    "    llm_struct = llm.with_structured_output(SensitivityLabel)  # devuelve instancia Pydantic\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", TEACHER_SYSTEM),\n",
    "        (\"user\", TEACHER_USER_TEMPLATE),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm_struct\n",
    "    return chain.invoke({\"doc_text\": doc_text})\n",
    "\n",
    "# Ejecuta teacher:\n",
    "# teacher_out = teacher_label_with_langchain(raw_text)\n",
    "# print(teacher_out.model_dump())\n",
    "\n",
    "print(\"Descomenta para ejecutar la llamada real al teacher.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa69d63",
   "metadata": {},
   "source": [
    "## 6) Guardar artifacts del teacher (versionado)\n",
    "Guardamos:\n",
    "- texto extraído\n",
    "- label del teacher\n",
    "- metadata de ejecución (modelo, prompt_version, hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f49bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_VERSION = \"teacher_sensitivity_v1\"\n",
    "\n",
    "def save_teacher_artifacts(doc_text: str, teacher_out: SensitivityLabel) -> None:\n",
    "    (ARTIFACTS_DIR / \"inputs\").mkdir(exist_ok=True)\n",
    "    (ARTIFACTS_DIR / \"teacher\").mkdir(exist_ok=True)\n",
    "\n",
    "    (ARTIFACTS_DIR / \"inputs\" / f\"{DOC_HASH}.txt\").write_text(doc_text, encoding=\"utf-8\")\n",
    "\n",
    "    meta = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"doc_hash\": DOC_HASH,\n",
    "        \"pdf_path\": str(PDF_PATH),\n",
    "        \"prompt_version\": PROMPT_VERSION,\n",
    "        \"teacher_model\": TEACHER_MODEL,\n",
    "        \"created_at\": datetime.datetime.now().isoformat(),\n",
    "    }\n",
    "    (ARTIFACTS_DIR / \"teacher\" / f\"{DOC_HASH}.meta.json\").write_text(\n",
    "        json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n",
    "    )\n",
    "    (ARTIFACTS_DIR / \"teacher\" / f\"{DOC_HASH}.label.json\").write_text(\n",
    "        json.dumps(teacher_out.model_dump(), indent=2, ensure_ascii=False), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# Ejemplo:\n",
    "# save_teacher_artifacts(raw_text, teacher_out)\n",
    "print(\"OK. Cuando tengas teacher_out, llama save_teacher_artifacts(raw_text, teacher_out).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10c2b2",
   "metadata": {},
   "source": [
    "## 7) Dataset JSONL para entrenar el SLM local (student)\n",
    "Generamos un ejemplo SFT tipo `messages + response`.\n",
    "Esto lo consumes con tu tool de fine-tuning (LoRA) en local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a739b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_SYSTEM = \"Eres un clasificador binario de sensibilidad documental. Responde SOLO JSON válido.\"\n",
    "STUDENT_USER_TEMPLATE = \"\"\"Clasifica sensibilidad (0=no sensible,1=sensible) según:\n",
    "- 1 si hay IBAN, NIF/CIF, datos bancarios, datos de pago, identificadores fiscales o info financiera identificable.\n",
    "- 0 si no.\n",
    "Devuelve SOLO JSON con: label, confidence, signals, rationale_short.\n",
    "\n",
    "Texto:\n",
    "<<<\n",
    "{doc_text}\n",
    ">>>\n",
    "\"\"\"\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def build_sft_example(doc_text: str, teacher_out: SensitivityLabel) -> dict:\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": STUDENT_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": STUDENT_USER_TEMPLATE.format(doc_text=doc_text)},\n",
    "        ],\n",
    "        \"response\": json.dumps(teacher_out.model_dump(), ensure_ascii=False),\n",
    "        \"meta\": {\n",
    "            \"doc_hash\": DOC_HASH,\n",
    "            \"teacher_model\": TEACHER_MODEL,\n",
    "            \"prompt_version\": PROMPT_VERSION,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Ejemplo:\n",
    "# train_path = ARTIFACTS_DIR / \"student_train.jsonl\"\n",
    "# append_jsonl(train_path, build_sft_example(raw_text, teacher_out))\n",
    "print(\"OK. JSONL listo cuando tengas teacher_out.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff807ab",
   "metadata": {},
   "source": [
    "## 8) Student con LangChain (ChatOllama) en modo JSON\n",
    "Usamos `langchain-ollama`.\n",
    "Para forzar JSON, pasamos `format='json'` como parámetro del modelo (Ollama JSON mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3623c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def student_predict_with_langchain(doc_text: str) -> SensitivityLabel:\n",
    "    # ChatOllama (Ollama) - JSON mode\n",
    "    # En Ollama, 'format': 'json' fuerza salida JSON.\n",
    "    llm = ChatOllama(\n",
    "        model=STUDENT_MODEL,\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        temperature=0,\n",
    "        format=\"json\",\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", STUDENT_SYSTEM),\n",
    "        (\"user\", STUDENT_USER_TEMPLATE),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    msg = chain.invoke({\"doc_text\": doc_text[:4000]})  # opcional truncar\n",
    "    # msg.content es string JSON\n",
    "    return SensitivityLabel.model_validate_json(msg.content)\n",
    "\n",
    "# Ejecuta student:\n",
    "# student_out = student_predict_with_langchain(raw_text)\n",
    "# print(student_out.model_dump())\n",
    "\n",
    "print(\"Descomenta para ejecutar. Requiere Ollama corriendo y el modelo instalado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf6ada",
   "metadata": {},
   "source": [
    "## 9) Knowledge Base Log (JSONL + SQLite opcional)\n",
    "Registramos cada ejecución como un 'evento' en una base de conocimiento:\n",
    "- hash documento\n",
    "- teacher y student\n",
    "- versiones\n",
    "- artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4575aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_EVENTS = KB_DIR / \"events.jsonl\"\n",
    "KB_DB = KB_DIR / \"kb.sqlite\"\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def log_kb_event(teacher_out: SensitivityLabel, student_out: Optional[SensitivityLabel] = None, notes: Optional[str] = None) -> dict:\n",
    "    event = {\n",
    "        \"ts\": datetime.datetime.now().isoformat(),\n",
    "        \"doc_hash\": DOC_HASH,\n",
    "        \"pdf_path\": str(PDF_PATH),\n",
    "        \"teacher\": teacher_out.model_dump(),\n",
    "        \"student\": student_out.model_dump() if student_out else None,\n",
    "        \"versions\": {\n",
    "            \"prompt_version\": PROMPT_VERSION,\n",
    "            \"teacher_model\": TEACHER_MODEL,\n",
    "            \"student_model\": STUDENT_MODEL,\n",
    "            \"ollama_base_url\": OLLAMA_BASE_URL,\n",
    "        },\n",
    "        \"artifacts_dir\": str(ARTIFACTS_DIR),\n",
    "        \"notes\": notes,\n",
    "    }\n",
    "    append_jsonl(KB_EVENTS, event)\n",
    "    return event\n",
    "\n",
    "def init_kb_db():\n",
    "    con = sqlite3.connect(str(KB_DB))\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS events (\n",
    "        ts TEXT,\n",
    "        doc_hash TEXT,\n",
    "        pdf_path TEXT,\n",
    "        prompt_version TEXT,\n",
    "        teacher_model TEXT,\n",
    "        student_model TEXT,\n",
    "        teacher_json TEXT,\n",
    "        student_json TEXT,\n",
    "        artifacts_dir TEXT,\n",
    "        notes TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_doc_hash ON events(doc_hash)\")\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "def insert_kb_db(event: dict):\n",
    "    con = sqlite3.connect(str(KB_DB))\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"INSERT INTO events VALUES (?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
    "        (\n",
    "            event[\"ts\"],\n",
    "            event[\"doc_hash\"],\n",
    "            event[\"pdf_path\"],\n",
    "            event[\"versions\"][\"prompt_version\"],\n",
    "            event[\"versions\"][\"teacher_model\"],\n",
    "            event[\"versions\"][\"student_model\"],\n",
    "            json.dumps(event[\"teacher\"], ensure_ascii=False),\n",
    "            json.dumps(event[\"student\"], ensure_ascii=False) if event[\"student\"] else None,\n",
    "            event[\"artifacts_dir\"],\n",
    "            event.get(\"notes\"),\n",
    "        ),\n",
    "    )\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "init_kb_db()\n",
    "print(\"KB ready:\", KB_EVENTS.resolve(), \"|\", KB_DB.resolve())\n",
    "\n",
    "# Ejemplo completo (cuando tengas teacher_out y student_out):\n",
    "# event = log_kb_event(teacher_out, student_out, notes=\"run demo\")\n",
    "# insert_kb_db(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc069968",
   "metadata": {},
   "source": [
    "## 10) Ejecución end-to-end (pasos)\n",
    "1) Extraer texto (ya hecho)\n",
    "2) Teacher:\n",
    "   - `teacher_out = teacher_label_with_langchain(raw_text)`\n",
    "   - `save_teacher_artifacts(raw_text, teacher_out)`\n",
    "3) Dataset:\n",
    "   - `append_jsonl(ARTIFACTS_DIR/'student_train.jsonl', build_sft_example(raw_text, teacher_out))`\n",
    "4) Student:\n",
    "   - `student_out = student_predict_with_langchain(raw_text)`\n",
    "5) KB:\n",
    "   - `event = log_kb_event(teacher_out, student_out)`\n",
    "   - `insert_kb_db(event)`\n",
    "\n",
    "Entrenamiento local (LoRA) del student:\n",
    "- usa el JSONL generado con una tool de fine-tuning.\n",
    "- luego crea un modelo en Ollama (Modelfile) y repite el paso 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
